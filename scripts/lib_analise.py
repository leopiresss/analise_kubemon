import pickle
import os

from sklearn import datasets

nome_dataset_default = 'svm'
def get_info_modelo(nome_dataset = nome_dataset_default):
    if nome_dataset == 'svm':
        return {
            'nome_dataset': 'svm',
            'parametros': {
                'arq_dataset_csv': '../dataset/svm.csv',
                'arq_dataset_pkl': '../dataset/svm.pkl'
            }
        }
    if nome_dataset == 'terasort':
        return {
            'nome_dataset': 'terasort',
            'parametros': {
                'arq_dataset_csv': '../dataset/terasort.csv',
                'arq_dataset_pkl': '../dataset/terasort.pkl'
            }
        }


def save_informacao_analise(datasets = None, nome_data_set = nome_dataset_default):
    try:
        info_modelo = get_info_modelo(nome_data_set)
        
        # Verificar se o arquivo PKL existe
        arquivo_pkl = info_modelo['parametros']['arq_dataset_pkl']
        
        with open(arquivo_pkl, 'wb') as f:
            datasets['X_train_scaled'] = datasets['X_train_scaled'] if 'X_train_scaled' in datasets else None
            datasets['X_test_scaled'] = datasets['X_test_scaled'] if 'X_test_scaled' in datasets else None
            datasets['X_val_scaled'] = datasets['X_val_scaled'] if 'X_val_scaled' in datasets else None            
            pickle.dump(datasets, f)
        print(f"âœ… Dataset salvo com sucesso em {arquivo_pkl}")
        
    except Exception as e:
        print(f"âŒ Erro ao salvar dataset: {e}")
        raise
    
def print_informacao_analise(nome_data_set = nome_dataset_default):    
    try:
        info_modelo = get_info_modelo(nome_data_set)
        
        # Verificar se o arquivo PKL existe
        arquivo_pkl = info_modelo['parametros']['arq_dataset_pkl']
        
        with open(arquivo_pkl, 'rb') as f:
            datasets = pickle.load(f)
            print('X_train.shape', datasets['X_train'].shape)
            print('X_test.shape', datasets['X_test'].shape)
            print('X_val.shape', datasets['X_val'].shape)            
            print('X_train_scaled.shape', datasets['X_train_scaled'].shape if 'X_train_scaled' in datasets and datasets['X_train_scaled'] is not None else None)
            print('X_test_scaled.shape', datasets['X_test_scaled'].shape if 'X_test_scaled' in datasets and datasets['X_test_scaled'] is not None else None)
            print('X_val_scaled.shape', datasets['X_val_scaled'].shape if 'X_val_scaled' in datasets and datasets['X_val_scaled'] is not None else None)
            print('classes_mapping', datasets['classes_mapping'] if 'classes_mapping' in datasets else None)
            print('features_ganho_informacao', datasets['features_ganho_informacao'] if 'features_ganho_informacao' in datasets else None)  
            print('qtd features_ganho_informacao: ', len(datasets['features_ganho_informacao']) if 'features_ganho_informacao' in datasets else None)  
        return datasets
        
    except FileNotFoundError as e:
        print(f"âŒ Erro de arquivo: {e}")
        raise
    except pickle.UnpicklingError as e:
        print(f"âŒ Erro ao carregar o arquivo PKL: {e}")
        raise
    except Exception as e:
        print(f"âŒ Erro inesperado ao carregar dataset: {e}")
        raise

def get_dataset_analise(nome_data_set = nome_dataset_default,analise_ganho_de_informacao=False):    
    try:
        info_modelo = get_info_modelo(nome_data_set)
        
        # Verificar se o arquivo PKL existe
        arquivo_pkl = info_modelo['parametros']['arq_dataset_pkl']
        
        with open(arquivo_pkl, 'rb') as f:
            datasets = pickle.load(f)
            
            if analise_ganho_de_informacao:
                if 'features_ganho_informacao' not in datasets or datasets['features_ganho_informacao'] is None:
                    raise ValueError("O dataset nÃ£o contÃ©m 'features_ganho_informacao' ou ela Ã© None. Execute primeiro a anÃ¡lise de ganho de informaÃ§Ã£o no notebook correspondente.")
                print("-------------",datasets['X_train'][datasets['features_ganho_informacao']])
                # Aplicar seleÃ§Ã£o de features baseada no ganho de informaÃ§Ã£o
                datasets['X_train'] = datasets['X_train'][datasets['features_ganho_informacao']]
                datasets['X_test'] = datasets['X_test'][datasets['features_ganho_informacao']]
                datasets['X_val'] = datasets['X_val'][datasets['features_ganho_informacao']]
                datasets['X_train_scaled'] = datasets['X_train_scaled'][datasets['features_ganho_informacao'] if 'X_train_scaled' in datasets else []]
                datasets['X_test_scaled'] = datasets['X_test_scaled'][datasets['features_ganho_informacao'] if 'X_test_scaled' in datasets else []]
                datasets['X_val_scaled'] = datasets['X_val_scaled'][datasets['features_ganho_informacao'] if 'X_val_scaled' in datasets else []]
                print("-------------",datasets['X_train'][datasets['features_ganho_informacao']])
        return datasets
        
    except FileNotFoundError as e:
        print(f"âŒ Erro de arquivo: {e}")
        raise
    except pickle.UnpicklingError as e:
        print(f"âŒ Erro ao carregar o arquivo PKL: {e}")
        raise
    except KeyError as e:
        print(f"âŒ Erro: Chave nÃ£o encontrada no dataset: {e}")
        raise
    except Exception as e:
        print(f"âŒ Erro inesperado ao carregar dataset: {e}")
        raise


def atualizar_features_dataset_analise(datasets = None,features = None):    
    if datasets is None or features is None:
        raise ValueError("Os parÃ¢metros 'datasets' e 'features' nÃ£o podem ser None.")
    # Aplicar seleÃ§Ã£o de features baseada no ganho de informaÃ§Ã£o
    datasets['X_train'] = datasets['X_train'][features]
    datasets['X_test'] = datasets['X_test'][features]
    datasets['X_val'] = datasets['X_val'][features]
    datasets['features_ganho_informacao'] = features
    return datasets




def main():
    """
    FunÃ§Ã£o principal para demonstrar o uso da biblioteca lib_analise
    """
    print("ğŸ” Biblioteca de AnÃ¡lise KubeMon")
    print("=" * 50)
    
    try:
        # Obter informaÃ§Ãµes do modelo padrÃ£o
        info_modelo = get_info_modelo()
        print(f"ğŸ“Š Dataset: {info_modelo['nome_dataset']}")
        print(f"ğŸ“ Arquivo CSV: {info_modelo['parametros']['arq_dataset_csv']}")
        print(f"ğŸ“¦ Arquivo PKL: {info_modelo['parametros']['arq_dataset_pkl']}")
        
        # Carregar dataset bÃ¡sico
        print("\nğŸ”„ Carregando dataset bÃ¡sico...")
        datasets_basico = get_dataset_analise(analise_ganho_de_informacao=False)
        
        print(f"âœ… Dataset bÃ¡sico carregado:")
        print(f"   â€¢ X_train shape: {datasets_basico['X_train'].shape}")
        print(f"   â€¢ X_test shape: {datasets_basico['X_test'].shape}")
        print(f"   â€¢ X_val shape: {datasets_basico['X_val'].shape}")
        print(f"   â€¢ Classes: {list(datasets_basico['classes_mapping'].keys())}")
        
        # Tentar carregar dataset com anÃ¡lise de ganho de informaÃ§Ã£o
        print("\nğŸ§  Tentando carregar dataset com anÃ¡lise de ganho de informaÃ§Ã£o...")
        try:
            datasets_gi = get_dataset_analise(analise_ganho_de_informacao=True)
            print(f"âœ… Dataset com ganho de informaÃ§Ã£o carregado:")
            print(f"   â€¢ Features selecionadas: {len(datasets_gi['features_ganho_informacao'])}")
            print(f"   â€¢ X_train shape reduzido: {datasets_gi['X_train'].shape}")
        except ValueError as e:
            print(f"âš ï¸ {e}")
        
    except Exception as e:
        print(f"âŒ Erro: {e}")
        print("   Verifique se os arquivos de dataset estÃ£o disponÃ­veis.")
    
    print("\nğŸ¯ Biblioteca pronta para uso!")


if __name__ == "__main__":
    main()
