{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57daeb27",
   "metadata": {},
   "source": [
    "# M√©tricas de Avalia√ß√£o - Dataset SVM KubeMon\n",
    "\n",
    "Este notebook prepara os dados a serem utilizados para os modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5f15b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Bibliotecas importadas com sucesso!\n",
      "üéØ Pronto para avaliar m√©tricas do modelo SVM\n"
     ]
    }
   ],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"üìö Bibliotecas importadas com sucesso!\")\n",
    "print(\"üéØ Pronto para avaliar m√©tricas do modelo SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52712541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vari√°veis globais\n",
    "import pickle\n",
    "import lib_analise \n",
    "info_modelo = lib_analise.get_info_modelo()  # para garantir que a fun√ß√£o est√° carregada da\n",
    "\n",
    "nome_dataset = info_modelo['nome_dataset']\n",
    "arq_dataset_csv = info_modelo['parametros']['arq_dataset_csv']\n",
    "arq_dataset_pkl = info_modelo['parametros']['arq_dataset_pkl']\n",
    "\n",
    "#carregar os datasets de teste, treino e valida√ß√£o do arquivo pickle\n",
    "with open(arq_dataset_pkl, 'rb') as f:\n",
    "    datasets = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a00b5",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec7058be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando dataset svm.csv...\n",
      "‚úÖ Dataset carregado com sucesso!\n",
      "üìä Dimens√µes: 80,648 linhas √ó 126 colunas\n",
      "\n",
      "üìã Informa√ß√µes do Target:\n",
      "   ‚Ä¢ Classes: {'interf': np.int64(45660), 'normal': np.int64(34988)}\n",
      "   ‚Ä¢ interf: 45,660 (56.6%)\n",
      "   ‚Ä¢ normal: 34,988 (43.4%)\n",
      "\n",
      "üìä Informa√ß√µes Gerais:\n",
      "   ‚Ä¢ Valores ausentes no target: 0\n",
      "   ‚Ä¢ Total de features: 125\n",
      "   ‚Ä¢ Balanceamento: Desbalanceado\n"
     ]
    }
   ],
   "source": [
    "# 1) Carregamento e higieniza√ß√£o (por dataset)\n",
    "\n",
    "# Carregar o dataset\n",
    "print(\"üìÇ Carregando dataset svm.csv...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(arq_dataset_csv)\n",
    "    print(f\"‚úÖ Dataset carregado com sucesso!\")\n",
    "    print(f\"üìä Dimens√µes: {df.shape[0]:,} linhas √ó {df.shape[1]} colunas\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao carregar dataset: {e}\")\n",
    "    print(\"üîÑ Tentando carregar uma amostra...\")\n",
    "    try:\n",
    "        df = pd.read_csv(arq_dataset_csv, nrows=10000)\n",
    "        print(f\"‚úÖ Amostra carregada: {df.shape[0]:,} linhas\")\n",
    "    except:\n",
    "        raise Exception(\"N√£o foi poss√≠vel carregar o dataset\")\n",
    "\n",
    "# Verificar se a coluna target existe\n",
    "if 'target' not in df.columns:\n",
    "    print(\"‚ùå Coluna 'target' n√£o encontrada!\")\n",
    "    print(f\"Colunas dispon√≠veis: {list(df.columns)}\")\n",
    "    raise Exception(\"Coluna target n√£o encontrada\")\n",
    "\n",
    "print(f\"\\nüìã Informa√ß√µes do Target:\")\n",
    "target_counts = df['target'].value_counts()\n",
    "print(f\"   ‚Ä¢ Classes: {dict(target_counts)}\")\n",
    "for classe, count in target_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   ‚Ä¢ {classe}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Mostrar informa√ß√µes gerais\n",
    "print(f\"\\nüìä Informa√ß√µes Gerais:\")\n",
    "print(f\"   ‚Ä¢ Valores ausentes no target: {df['target'].isnull().sum()}\")\n",
    "print(f\"   ‚Ä¢ Total de features: {df.shape[1] - 1}\")\n",
    "print(f\"   ‚Ä¢ Balanceamento: {'Balanceado' if target_counts.min() / target_counts.max() > 0.8 else 'Desbalanceado'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36916825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preparando dados para treinamento...\n",
      "üìä Features para o modelo:\n",
      "   ‚Ä¢ Total de features: 122\n",
      "   ‚Ä¢ Colunas exclu√≠das: ['os_timestamp', 'node_name', 'iteration', 'target']\n",
      "   ‚Ä¢ Target codificado: {'interf': np.int64(0), 'normal': np.int64(1)}\n",
      "\n",
      "‚úÖ Dados preparados:\n",
      "   ‚Ä¢ Shape X: (80648, 122)\n",
      "   ‚Ä¢ Shape y: 80648\n",
      "   ‚Ä¢ Classes √∫nicas: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Prepara√ß√£o dos dados\n",
    "print(\"üîß Preparando dados para treinamento...\")\n",
    "\n",
    "# Separar features e target\n",
    "colunas_excluir = ['os_timestamp', 'node_name', 'iteration', 'target']\n",
    "colunas_excluir = [col for col in colunas_excluir if col in df.columns]\n",
    "\n",
    "# Selecionar apenas features num√©ricas\n",
    "features_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "features_para_modelo = [col for col in features_numericas if col not in colunas_excluir]\n",
    "\n",
    "print(f\"üìä Features para o modelo:\")\n",
    "print(f\"   ‚Ä¢ Total de features: {len(features_para_modelo)}\")\n",
    "print(f\"   ‚Ä¢ Colunas exclu√≠das: {colunas_excluir}\")\n",
    "\n",
    "# Preparar X e y\n",
    "X = df[features_para_modelo].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "# Tratar valores ausentes\n",
    "valores_ausentes = X.isnull().sum().sum()\n",
    "if valores_ausentes > 0:\n",
    "    print(f\"   ‚Ä¢ Preenchendo {valores_ausentes:,} valores ausentes com a mediana...\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# Codificar target se necess√°rio\n",
    "le = LabelEncoder()\n",
    "if y.dtype == 'object':\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    classes_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(f\"   ‚Ä¢ Target codificado: {classes_mapping}\")\n",
    "else:\n",
    "    y_encoded = y.values\n",
    "    classes_mapping = None\n",
    "\n",
    "print(f\"\\n‚úÖ Dados preparados:\")\n",
    "print(f\"   ‚Ä¢ Shape X: {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Shape y: {len(y_encoded)}\")\n",
    "print(f\"   ‚Ä¢ Classes √∫nicas: {np.unique(y_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19dec0",
   "metadata": {},
   "source": [
    "## 2. Divis√£o dos Dados e Normaliza√ß√£o\n",
    "# Utilizando Yeo-Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0c29810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dividindo dados em treino e teste...\n",
      "üìä Divis√£o dos dados:\n",
      "   ‚Ä¢ Treino: 32,259 amostras (40.0%)\n",
      "   ‚Ä¢ Teste:  24,194 amostras (30.0%)\n",
      "   ‚Ä¢ Valida√ß√£o: 24,195 amostras (30.0%)\n",
      "\n",
      "üìã Distribui√ß√£o das classes:\n",
      "   ‚Ä¢ Classe 0: Treino 56.6% | Teste 56.6% | Valida√ß√£o 56.6%\n",
      "   ‚Ä¢ Classe 1: Treino 43.4% | Teste 43.4% | Valida√ß√£o 43.4%\n",
      "\n",
      "‚öñÔ∏è Aplicando transforma√ß√£o Yeo-Johnson...\n",
      "   ‚úÖ Transforma√ß√£o Yeo-Johnson aplicada com StandardScaler integrado\n",
      "   ‚Ä¢ M√©dia treino antes: 970513.048 | depois: -0.000\n",
      "   ‚Ä¢ Std treino antes: 4262885.915 | depois: 0.805\n",
      "   ‚Ä¢ M√©dia teste antes: 986004.100 | depois: -0.000\n",
      "   ‚Ä¢ Std teste antes: 4481052.608 | depois: 1.453\n",
      "   ‚Ä¢ M√©dia valida√ß√£o antes: 982812.774 | depois: -0.001\n",
      "   ‚Ä¢ Std valida√ß√£o antes: 4428113.690 | depois: 0.793\n",
      "   ‚Ä¢ Transforma√ß√£o aplicada: Yeo-Johnson + Padroniza√ß√£o\n"
     ]
    }
   ],
   "source": [
    "# 3) Particionamento estratificado 40/30/30\n",
    "\n",
    "# Divis√£o treino/teste\n",
    "print(\"üìä Dividindo dados em treino e teste...\")\n",
    "\n",
    "# Dividindo o teste mantendo a propor√ß√£o das classes (stratify) em 40% treino e 60% teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.6, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Dividindo o teste em teste e valida√ß√£o (50%/50%)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, \n",
    "    test_size=0.5, \n",
    "    random_state=42, \n",
    "    stratify=y_test\n",
    ")\n",
    "\n",
    "print(f\"üìä Divis√£o dos dados:\")\n",
    "print(f\"   ‚Ä¢ Treino: {X_train.shape[0]:,} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Teste:  {X_test.shape[0]:,} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Valida√ß√£o: {X_val.shape[0]:,} amostras ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verificar distribui√ß√£o das classes\n",
    "print(f\"\\nüìã Distribui√ß√£o das classes:\")\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
    "\n",
    "for classe in unique_train:\n",
    "    train_pct = (counts_train[unique_train == classe][0] / len(y_train)) * 100\n",
    "    test_pct = (counts_test[unique_test == classe][0] / len(y_test)) * 100\n",
    "    val_pct = (counts_val[unique_val == classe][0] / len(y_val)) * 100\n",
    "    print(f\"   ‚Ä¢ Classe {classe}: Treino {train_pct:.1f}% | Teste {test_pct:.1f}% | Valida√ß√£o {val_pct:.1f}%\")\n",
    "\n",
    "# Normaliza√ß√£o dos dados usando Yeo-Johnson\n",
    "print(f\"\\n‚öñÔ∏è Aplicando transforma√ß√£o Yeo-Johnson...\")\n",
    "yeo_johnson_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "# O fit √© feito apenas no conjunto de treino para evitar data leakage\n",
    "X_train_scaled = yeo_johnson_transformer.fit_transform(X_train)\n",
    "X_test_scaled = yeo_johnson_transformer.transform(X_test)\n",
    "X_val_scaled = yeo_johnson_transformer.transform(X_val)\n",
    "\n",
    "print(f\"   ‚úÖ Transforma√ß√£o Yeo-Johnson aplicada com StandardScaler integrado\")\n",
    "print(f\"   ‚Ä¢ M√©dia treino antes: {X_train.mean().mean():.3f} | depois: {X_train_scaled.mean():.3f}\")\n",
    "print(f\"   ‚Ä¢ Std treino antes: {X_train.std().mean():.3f} | depois: {X_train_scaled.std().mean():.3f}\")\n",
    "print(f\"   ‚Ä¢ M√©dia teste antes: {X_test.mean().mean():.3f} | depois: {X_test_scaled.mean():.3f}\")\n",
    "print(f\"   ‚Ä¢ Std teste antes: {X_test.std().mean():.3f} | depois: {X_test_scaled.std().mean():.3f}\")\n",
    "print(f\"   ‚Ä¢ M√©dia valida√ß√£o antes: {X_val.mean().mean():.3f} | depois: {X_val_scaled.mean():.3f}\")\n",
    "print(f\"   ‚Ä¢ Std valida√ß√£o antes: {X_val.std().mean():.3f} | depois: {X_val_scaled.std().mean():.3f}\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Transforma√ß√£o aplicada: Yeo-Johnson + Padroniza√ß√£o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1e3d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets salvos em arquivos pickle com sucesso!\n"
     ]
    }
   ],
   "source": [
    "## 3. Gera pickles com os datasets e grava um arquivo\n",
    "# Salvar datasets em arquivos pickle gera um √∫nico arquivo para todos datasets\n",
    "import pickle  \n",
    "\n",
    "with open(arq_dataset_pkl, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'X_val': X_val,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'y_val': y_val,\n",
    "        'X_train_scaled': X_train_scaled,\n",
    "        'X_test_scaled': X_test_scaled,\n",
    "        'X_val_scaled': X_val_scaled,\n",
    "        'classes_mapping': classes_mapping, \n",
    "        'features_ganho_informacao': features_para_modelo\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Datasets salvos em arquivos pickle com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb6821ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregar os datasets de teste, treino e valida√ß√£o do arquivo pickle\n",
    "with open(arq_dataset_pkl, 'rb') as f:\n",
    "    datasets = pickle.load(f)\n",
    "\n",
    "X_train = datasets['X_train']\n",
    "X_test = datasets['X_test']\n",
    "X_val = datasets['X_val']\n",
    "y_train = datasets['y_train']\n",
    "y_test = datasets['y_test']\n",
    "y_val = datasets['y_val']\n",
    "X_train_scaled = datasets['X_train_scaled']\n",
    "X_test_scaled = datasets['X_test_scaled']\n",
    "X_val_scaled = datasets['X_val_scaled']\n",
    "classes_mapping = datasets['classes_mapping']\n",
    "features_ganho_informacao  = datasets['features_ganho_informacao']\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
