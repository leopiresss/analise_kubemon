{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57daeb27",
   "metadata": {},
   "source": [
    "# Métricas de Avaliação - Dataset SVM KubeMon\n",
    "\n",
    "Este notebook prepara os dados a serem utilizados para os modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f283d715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from seaborn) (2.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leopi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "675fe5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy version: 1.16.2\n",
      "sklearn version: 1.7.2\n",
      "numpy version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "print(f\"scipy version: {scipy.__version__}\")\n",
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5f15b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52712541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Bibliotecas importadas com sucesso!\n",
      "🎯 Pronto para avaliar métricas do arquivo de dataset svm\n"
     ]
    }
   ],
   "source": [
    "#Variáveis globais\n",
    "import pickle\n",
    "import lib_analise \n",
    "info_modelo = lib_analise.get_info_modelo()  # para garantir que a função está carregada da\n",
    "\n",
    "nome_dataset = info_modelo['nome_dataset']\n",
    "arq_dataset_csv = info_modelo['parametros']['arq_dataset_csv']\n",
    "arq_dataset_pkl = info_modelo['parametros']['arq_dataset_pkl']\n",
    "\n",
    "print(\"📚 Bibliotecas importadas com sucesso!\")\n",
    "print(f\"🎯 Pronto para avaliar métricas do arquivo de dataset {nome_dataset}\")\n",
    "#datasets = lib_analise.get_dataset_analise(analise_ganho_de_informacao=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a00b5",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec7058be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dataset svm.csv...\n",
      "✅ Dataset carregado com sucesso!\n",
      "📊 Dimensões: 80,648 linhas × 126 colunas\n",
      "\n",
      "📋 Informações do Target:\n",
      "   • Classes: {'interf': np.int64(45660), 'normal': np.int64(34988)}\n",
      "   • interf: 45,660 (56.6%)\n",
      "   • normal: 34,988 (43.4%)\n",
      "\n",
      "📊 Informações Gerais:\n",
      "   • Total de amostras: 80,648\n",
      "   • Total de features: 125\n",
      "   • Valores ausentes no target: 0\n",
      "\n",
      "✅ Balanceamento do Dataset:\n",
      "   • Status: ✅ BALANCEADO\n",
      "   • Razão: 1.31:1\n",
      "\n",
      "   Distribuição por classe:\n",
      "   • Classe interf: 45,660 ( 56.6%) ████████████████████████████\n",
      "   • Classe normal: 34,988 ( 43.4%) █████████████████████\n",
      "\n",
      "   💡 Recomendação: Treinar normalmente\n"
     ]
    }
   ],
   "source": [
    "# 1) Carregamento e higienização (por dataset)\n",
    "\n",
    "# Carregar o dataset\n",
    "print(\"📂 Carregando dataset svm.csv...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(arq_dataset_csv)\n",
    "    print(f\"✅ Dataset carregado com sucesso!\")\n",
    "    print(f\"📊 Dimensões: {df.shape[0]:,} linhas × {df.shape[1]} colunas\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao carregar dataset: {e}\")\n",
    "    print(\"🔄 Tentando carregar uma amostra...\")\n",
    "    try:\n",
    "        df = pd.read_csv(arq_dataset_csv, nrows=10000)\n",
    "        print(f\"✅ Amostra carregada: {df.shape[0]:,} linhas\")\n",
    "    except:\n",
    "        raise Exception(\"Não foi possível carregar o dataset\")\n",
    "\n",
    "# Verificar se a coluna target existe\n",
    "if 'target' not in df.columns:\n",
    "    print(\"❌ Coluna 'target' não encontrada!\")\n",
    "    print(f\"Colunas disponíveis: {list(df.columns)}\")\n",
    "    raise Exception(\"Coluna target não encontrada\")\n",
    "\n",
    "print(f\"\\n📋 Informações do Target:\")\n",
    "target_counts = df['target'].value_counts()\n",
    "print(f\"   • Classes: {dict(target_counts)}\")\n",
    "for classe, count in target_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   • {classe}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "\n",
    "# Calcular informações de balanceamento\n",
    "target_counts = df['target'].value_counts()\n",
    "target_percentages = df['target'].value_counts(normalize=True) * 100\n",
    "total_samples = len(df)\n",
    "\n",
    "# Calcular razão de desbalanceamento\n",
    "min_class = target_counts.min()\n",
    "max_class = target_counts.max()\n",
    "balance_ratio = max_class / min_class\n",
    "reverse_ratio = min_class / max_class  # Para comparação com limiar\n",
    "\n",
    "# Classificar nível de desbalanceamento\n",
    "if balance_ratio < 1.5:\n",
    "    balance_status = \"✅ BALANCEADO\"\n",
    "    balance_icon = \"✅\"\n",
    "    recommendation = \"Treinar normalmente\"\n",
    "elif balance_ratio < 3:\n",
    "    balance_status = \"⚠️ LEVEMENTE DESBALANCEADO\"\n",
    "    balance_icon = \"⚠️\"\n",
    "    recommendation = \"Considere usar class_weight='balanced'\"\n",
    "elif balance_ratio < 9:\n",
    "    balance_status = \"⚠️⚠️ MODERADAMENTE DESBALANCEADO\"\n",
    "    balance_icon = \"⚠️⚠️\"\n",
    "    recommendation = \"Recomendado: SMOTE ou class_weight\"\n",
    "else:\n",
    "    balance_status = \"❌ SEVERAMENTE DESBALANCEADO\"\n",
    "    balance_icon = \"❌\"\n",
    "    recommendation = \"CRÍTICO! Use técnicas de balanceamento\"\n",
    "\n",
    "# Mostrar informações gerais\n",
    "print(f\"\\n📊 Informações Gerais:\")\n",
    "print(f\"   • Total de amostras: {total_samples:,}\")\n",
    "print(f\"   • Total de features: {df.shape[1] - 1}\")\n",
    "print(f\"   • Valores ausentes no target: {df['target'].isnull().sum()}\")\n",
    "\n",
    "print(f\"\\n{balance_icon} Balanceamento do Dataset:\")\n",
    "print(f\"   • Status: {balance_status}\")\n",
    "print(f\"   • Razão: {balance_ratio:.2f}:1\")\n",
    "\n",
    "# Mostrar distribuição das classes\n",
    "print(f\"\\n   Distribuição por classe:\")\n",
    "for classe in sorted(target_counts.index):\n",
    "    count = target_counts[classe]\n",
    "    percentage = target_percentages[classe]\n",
    "    bar_length = int(percentage / 2)  # Barra visual\n",
    "    bar = \"█\" * bar_length\n",
    "    print(f\"   • Classe {classe}: {count:5,} ({percentage:5.1f}%) {bar}\")\n",
    "\n",
    "print(f\"\\n   💡 Recomendação: {recommendation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36916825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Preparando dados para treinamento...\n",
      "📊 Features para o modelo:\n",
      "   • Total de features: 122\n",
      "   • Colunas excluídas: ['os_timestamp', 'node_name', 'iteration', 'target']\n",
      "   • Target codificado: {'interf': np.int64(0), 'normal': np.int64(1)}\n",
      "\n",
      "✅ Dados preparados:\n",
      "   • Shape X: (80648, 122)\n",
      "   • Shape y: 80648\n",
      "   • Classes únicas: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Preparação dos dados\n",
    "print(\"🔧 Preparando dados para treinamento...\")\n",
    "\n",
    "# Separar features e target\n",
    "colunas_excluir = ['os_timestamp', 'node_name', 'iteration', 'target']\n",
    "colunas_excluir = [col for col in colunas_excluir if col in df.columns]\n",
    "\n",
    "# Selecionar apenas features numéricas\n",
    "features_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "features_para_modelo = [col for col in features_numericas if col not in colunas_excluir]\n",
    "\n",
    "print(f\"📊 Features para o modelo:\")\n",
    "print(f\"   • Total de features: {len(features_para_modelo)}\")\n",
    "print(f\"   • Colunas excluídas: {colunas_excluir}\")\n",
    "\n",
    "# Preparar X e y\n",
    "X = df[features_para_modelo].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "# Tratar valores ausentes\n",
    "valores_ausentes = X.isnull().sum().sum()\n",
    "if valores_ausentes > 0:\n",
    "    print(f\"   • Preenchendo {valores_ausentes:,} valores ausentes com a mediana...\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "\n",
    "# Codificar target se necessário\n",
    "le = LabelEncoder()\n",
    "if y.dtype == 'object':\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    classes_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(f\"   • Target codificado: {classes_mapping}\")\n",
    "else:\n",
    "    y_encoded = y.values\n",
    "    classes_mapping = None\n",
    "\n",
    "print(f\"\\n✅ Dados preparados:\")\n",
    "print(f\"   • Shape X: {X.shape}\")\n",
    "print(f\"   • Shape y: {len(y_encoded)}\")\n",
    "print(f\"   • Classes únicas: {np.unique(y_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19dec0",
   "metadata": {},
   "source": [
    "## 2. Divisão dos Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0c29810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dividindo dados em treino e teste...\n",
      "📊 Divisão dos dados:\n",
      "   • Treino: 32,259 amostras (40.0%)\n",
      "   • Teste:  24,194 amostras (30.0%)\n",
      "   • Validação: 24,195 amostras (30.0%)\n",
      "\n",
      "📋 Distribuição das classes:\n",
      "   • Classe 0: Treino 56.6% | Teste 56.6% | Validação 56.6%\n",
      "   • Classe 1: Treino 43.4% | Teste 43.4% | Validação 43.4%\n"
     ]
    }
   ],
   "source": [
    "# 3) Particionamento estratificado 40/30/30\n",
    "\n",
    "# Divisão treino/teste\n",
    "print(\"📊 Dividindo dados em treino e teste...\")\n",
    "\n",
    "# Dividindo o teste mantendo a proporção das classes (stratify) em 40% treino e 60% teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.6, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Dividindo o teste em teste e validação (50%/50%)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, \n",
    "    test_size=0.5, \n",
    "    random_state=42, \n",
    "    stratify=y_test\n",
    ")\n",
    "\n",
    "print(f\"📊 Divisão dos dados:\")\n",
    "print(f\"   • Treino: {X_train.shape[0]:,} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   • Teste:  {X_test.shape[0]:,} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   • Validação: {X_val.shape[0]:,} amostras ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verificar distribuição das classes\n",
    "print(f\"\\n📋 Distribuição das classes:\")\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
    "\n",
    "for classe in unique_train:\n",
    "    train_pct = (counts_train[unique_train == classe][0] / len(y_train)) * 100\n",
    "    test_pct = (counts_test[unique_test == classe][0] / len(y_test)) * 100\n",
    "    val_pct = (counts_val[unique_val == classe][0] / len(y_val)) * 100\n",
    "    print(f\"   • Classe {classe}: Treino {train_pct:.1f}% | Teste {test_pct:.1f}% | Validação {val_pct:.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1da1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregar os datasets de teste, treino e validação do arquivo pickle\n",
    "import lib_analise\n",
    "datasets = {}\n",
    "datasets['nome_dataset'] = nome_dataset\n",
    "datasets['X_train'] = X_train\n",
    "datasets['X_test'] = X_test\n",
    "datasets['X_val'] = X_val\n",
    "datasets['y_train'] = y_train\n",
    "datasets['y_test'] = y_test\n",
    "datasets['y_val'] = y_val\n",
    "datasets['classes_mapping'] = classes_mapping\n",
    "datasets['features_ganho_informacao'] = features_para_modelo\n",
    "# lib_analise.save_informacao_analise(datasets)\n",
    "datasets = lib_analise.print_informacao_analise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67f13f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(f\"\\n⚖️ Aplicando transformação Yeo-Johnson...\")\\nyeo_johnson_transformer = PowerTransformer(method=\\'yeo-johnson\\', standardize=True)\\n# O fit é feito apenas no conjunto de treino para evitar data leakage\\nX_train_scaled = yeo_johnson_transformer.fit_transform(X_train)\\nX_test_scaled = yeo_johnson_transformer.transform(X_test)\\nX_val_scaled = yeo_johnson_transformer.transform(X_val)\\n\\nprint(\\'X_train_scaled.shape\\', X_train_scaled.shape)\\nprint(\\'X_test_scaled.shape\\', X_test_scaled.shape)\\nprint(\\'X_val_scaled.shape\\', X_val_scaled.shape)\\n\\nprint(\\'X_train.shape\\', X_train.shape)\\nprint(\\'X_test.shape\\', X_test.shape)\\nprint(\\'X_val.shape\\', X_val.shape)\\nprint(f\"   ✅ Transformação Yeo-Johnson aplicada com StandardScaler integrado\")\\nprint(f\"   • Média treino antes: {X_train.mean().mean():.3f} | depois: {X_train_scaled.mean():.3f}\")\\nprint(f\"   • Std treino antes: {X_train.std().mean():.3f} | depois: {X_train_scaled.std().mean():.3f}\")\\nprint(f\"   • Média teste antes: {X_test.mean().mean():.3f} | depois: {X_test_scaled.mean():.3f}\")\\nprint(f\"   • Std teste antes: {X_test.std().mean():.3f} | depois: {X_test_scaled.std().mean():.3f}\")\\nprint(f\"   • Média validação antes: {X_val.mean().mean():.3f} | depois: {X_val_scaled.mean():.3f}\")\\nprint(f\"   • Std validação antes: {X_val.std().mean():.3f} | depois: {X_val_scaled.std().mean():.3f}\")\\n\\nprint(f\"   • Transformação aplicada: Yeo-Johnson + Padronização\")\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(f\"\\n⚖️ Aplicando transformação Yeo-Johnson...\")\n",
    "yeo_johnson_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "# O fit é feito apenas no conjunto de treino para evitar data leakage\n",
    "X_train_scaled = yeo_johnson_transformer.fit_transform(X_train)\n",
    "X_test_scaled = yeo_johnson_transformer.transform(X_test)\n",
    "X_val_scaled = yeo_johnson_transformer.transform(X_val)\n",
    "\n",
    "print('X_train_scaled.shape', X_train_scaled.shape)\n",
    "print('X_test_scaled.shape', X_test_scaled.shape)\n",
    "print('X_val_scaled.shape', X_val_scaled.shape)\n",
    "\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_test.shape', X_test.shape)\n",
    "print('X_val.shape', X_val.shape)\n",
    "print(f\"   ✅ Transformação Yeo-Johnson aplicada com StandardScaler integrado\")\n",
    "print(f\"   • Média treino antes: {X_train.mean().mean():.3f} | depois: {X_train_scaled.mean():.3f}\")\n",
    "print(f\"   • Std treino antes: {X_train.std().mean():.3f} | depois: {X_train_scaled.std().mean():.3f}\")\n",
    "print(f\"   • Média teste antes: {X_test.mean().mean():.3f} | depois: {X_test_scaled.mean():.3f}\")\n",
    "print(f\"   • Std teste antes: {X_test.std().mean():.3f} | depois: {X_test_scaled.std().mean():.3f}\")\n",
    "print(f\"   • Média validação antes: {X_val.mean().mean():.3f} | depois: {X_val_scaled.mean():.3f}\")\n",
    "print(f\"   • Std validação antes: {X_val.std().mean():.3f} | depois: {X_val_scaled.std().mean():.3f}\")\n",
    "\n",
    "print(f\"   • Transformação aplicada: Yeo-Johnson + Padronização\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae5126",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1e3d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datasets salvos em arquivos pickle com sucesso!\n"
     ]
    }
   ],
   "source": [
    "## 3. Gera pickles com os datasets e grava um arquivo\n",
    "# Salvar datasets em arquivos pickle gera um único arquivo para todos datasets\n",
    "import pickle  \n",
    "\n",
    "lib_analise.get_dataset_analise()\n",
    "lib_analise.print_informacao_analise()\n",
    "print(f\"✅ Datasets salvos em arquivos pickle com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb6821ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregar os datasets de teste, treino e validação do arquivo pickle\n",
    "with open(arq_dataset_pkl, 'rb') as f:\n",
    "    datasets = pickle.load(f)\n",
    "nome_dataset = datasets['nome_dataset']\n",
    "X_train = datasets['X_train']\n",
    "X_test = datasets['X_test']\n",
    "X_val = datasets['X_val']\n",
    "y_train = datasets['y_train']\n",
    "y_test = datasets['y_test']\n",
    "y_val = datasets['y_val']\n",
    "classes_mapping = datasets['classes_mapping']\n",
    "features_ganho_informacao  = datasets['features_ganho_informacao']\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
