{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0a7233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NumPy version: 1.24.4\n",
      "‚úÖ Pandas version: 1.5.3\n",
      "‚úÖ Scikit-learn version: 1.3.0\n",
      "‚úÖ Matplotlib version: 3.7.2\n",
      "‚úÖ Seaborn version: 0.12.2\n",
      "üéâ Todas as depend√™ncias est√£o funcionando corretamente!\n"
     ]
    }
   ],
   "source": [
    "# Verifica√ß√£o e instala√ß√£o de depend√™ncias\n",
    "import sys\n",
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "def check_and_install_packages():\n",
    "    \"\"\"\n",
    "    Verifica e instala pacotes necess√°rios se houver problemas de compatibilidade\n",
    "    \"\"\"\n",
    "    required_packages = [\n",
    "        'numpy>=1.24.0',\n",
    "        'pandas>=1.5.0',\n",
    "        'scikit-learn>=1.3.0',\n",
    "        'matplotlib>=3.7.0',\n",
    "        'seaborn>=0.12.0'\n",
    "    ]\n",
    "    \n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            if 'numpy' in package:\n",
    "                import numpy as np\n",
    "                print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
    "            elif 'pandas' in package:\n",
    "                import pandas as pd\n",
    "                print(f\"‚úÖ Pandas version: {pd.__version__}\")\n",
    "            elif 'scikit-learn' in package:\n",
    "                import sklearn\n",
    "                print(f\"‚úÖ Scikit-learn version: {sklearn.__version__}\")\n",
    "            elif 'matplotlib' in package:\n",
    "                import matplotlib\n",
    "                print(f\"‚úÖ Matplotlib version: {matplotlib.__version__}\")\n",
    "            elif 'seaborn' in package:\n",
    "                import seaborn as sns\n",
    "                print(f\"‚úÖ Seaborn version: {sns.__version__}\")\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ùå Erro ao importar {package}: {e}\")\n",
    "            print(f\"üîÑ Instalando {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    \n",
    "    print(\"üéâ Todas as depend√™ncias est√£o funcionando corretamente!\")\n",
    "\n",
    "# Executar verifica√ß√£o\n",
    "try:\n",
    "    check_and_install_packages()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Problema detectado: {e}\")\n",
    "    print(\"üîÑ Reinstalando NumPy especificamente...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"numpy==1.24.4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6d50a",
   "metadata": {},
   "source": [
    "# An√°lise de Ganho de Informa√ß√£o e Mutual Information\n",
    "\n",
    "Este notebook calcula o ganho de informa√ß√£o e mutual information para o dataset SVM.\n",
    "- **Ganho de Informa√ß√£o**: Mede a redu√ß√£o na entropia ap√≥s dividir o dataset com base em um atributo\n",
    "- **Mutual Information**: Mede a depend√™ncia m√∫tua entre duas vari√°veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed44bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e63ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vari√°veis globais\n",
    "arq_dataset_pkl = '../dataset/svm.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78816594",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#carregar os datasets de teste, treino e valida√ß√£o do arquivo pickle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(arq_dataset_pkl, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      5\u001b[0m X_train \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m X_test \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# Carregar os datasets de teste, treino e valida√ß√£o do arquivo pickle\n",
    "print(\"üìÇ Carregando datasets preprocessados...\")\n",
    "\n",
    "try:\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Verificar se o arquivo existe\n",
    "    import os\n",
    "    if not os.path.exists(arq_dataset_pkl):\n",
    "        print(f\"‚ùå Arquivo pickle n√£o encontrado: {arq_dataset_pkl}\")\n",
    "        print(\"üîÑ Executando carregamento direto do CSV...\")\n",
    "        # Carregar diretamente do CSV se o pickle n√£o existir\n",
    "        raise FileNotFoundError(\"Pickle n√£o encontrado\")\n",
    "    \n",
    "    # Carregar o arquivo pickle\n",
    "    with open(arq_dataset_pkl, 'rb') as f:\n",
    "        datasets = pickle.load(f)\n",
    "    \n",
    "    # Extrair os datasets\n",
    "    X_train = datasets['X_train']\n",
    "    X_test = datasets['X_test']\n",
    "    X_val = datasets['X_val']\n",
    "    y_train = datasets['y_train']\n",
    "    y_test = datasets['y_test']\n",
    "    y_val = datasets['y_val']\n",
    "    X_train_scaled = datasets['X_train_scaled']\n",
    "    X_test_scaled = datasets['X_test_scaled']\n",
    "    X_val_scaled = datasets['X_val_scaled']\n",
    "    classes_mapping = datasets['classes_mapping']\n",
    "    \n",
    "    print(\"‚úÖ Datasets carregados com sucesso!\")\n",
    "    print(f\"   ‚Ä¢ X_train shape: {X_train.shape}\")\n",
    "    print(f\"   ‚Ä¢ X_test shape: {X_test.shape}\")\n",
    "    print(f\"   ‚Ä¢ X_val shape: {X_val.shape}\")\n",
    "    print(f\"   ‚Ä¢ Classes mapping: {classes_mapping}\")\n",
    "    \n",
    "    # Usar os dados preprocessados para an√°lise\n",
    "    print(\"\\nüìä Usando dados preprocessados para an√°lise de ganho de informa√ß√£o...\")\n",
    "    \n",
    "    # Para o ganho de informa√ß√£o, vamos usar X_train e y_train\n",
    "    X_for_analysis = pd.DataFrame(X_train)\n",
    "    y_for_analysis = y_train\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Dados para an√°lise: {X_for_analysis.shape}\")\n",
    "    print(f\"   ‚Ä¢ Target para an√°lise: {len(y_for_analysis)} amostras\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao carregar pickle: {e}\")\n",
    "    print(\"üîÑ Carregando dados diretamente do CSV...\")\n",
    "    \n",
    "    # Fallback: carregar diretamente do CSV\n",
    "    try:\n",
    "        df = pd.read_csv('../dataset/svm.csv')\n",
    "        print(f\"‚úÖ Dataset CSV carregado: {df.shape}\")\n",
    "        \n",
    "        # Assumir que a √∫ltima coluna √© o target\n",
    "        X_for_analysis = df.iloc[:, :-1]\n",
    "        y_for_analysis = df.iloc[:, -1]\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Features: {X_for_analysis.shape}\")\n",
    "        print(f\"   ‚Ä¢ Target: {len(y_for_analysis)} amostras\")\n",
    "        \n",
    "    except Exception as csv_error:\n",
    "        print(f\"‚ùå Erro ao carregar CSV: {csv_error}\")\n",
    "        print(\"üõë N√£o foi poss√≠vel carregar os dados. Verifique os arquivos.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do dataset\n",
    "print(\"Carregando dataset SVM...\")\n",
    "df = pd.read_csv('../dataset/svm.csv')\n",
    "\n",
    "print(f\"Shape do dataset: {df.shape}\")\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "print(df.head())\n",
    "print(f\"\\nInforma√ß√µes do dataset:\")\n",
    "print(df.info())\n",
    "print(f\"\\nValores √∫nicos por coluna:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} valores √∫nicos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5df3ec",
   "metadata": {},
   "source": [
    "## Fun√ß√µes para C√°lculo de Ganho de Informa√ß√£o\n",
    "\n",
    "O ganho de informa√ß√£o √© calculado como:\n",
    "**IG(S, A) = H(S) - H(S|A)**\n",
    "\n",
    "Onde:\n",
    "- H(S) √© a entropia do conjunto original\n",
    "- H(S|A) √© a entropia condicional ap√≥s a divis√£o pelo atributo A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d63c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    \"\"\"\n",
    "    Calcula a entropia de um vetor de r√≥tulos\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Conta a frequ√™ncia de cada classe\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    \n",
    "    # Calcula a entropia\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "    return entropy_value\n",
    "\n",
    "def calculate_information_gain(X_feature, y):\n",
    "    \"\"\"\n",
    "    Calcula o ganho de informa√ß√£o para um atributo espec√≠fico\n",
    "    \"\"\"\n",
    "    # Entropia total do conjunto\n",
    "    total_entropy = calculate_entropy(y)\n",
    "    \n",
    "    # Valores √∫nicos do atributo\n",
    "    unique_values = np.unique(X_feature)\n",
    "    \n",
    "    # Entropia condicional\n",
    "    weighted_entropy = 0\n",
    "    for value in unique_values:\n",
    "        # √çndices onde o atributo tem esse valor\n",
    "        indices = X_feature == value\n",
    "        subset_y = y[indices]\n",
    "        \n",
    "        # Peso da subdivis√£o\n",
    "        weight = len(subset_y) / len(y)\n",
    "        \n",
    "        # Entropia da subdivis√£o\n",
    "        subset_entropy = calculate_entropy(subset_y)\n",
    "        \n",
    "        # Adiciona √† entropia ponderada\n",
    "        weighted_entropy += weight * subset_entropy\n",
    "    \n",
    "    # Ganho de informa√ß√£o\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def calculate_information_gain_ratio(X_feature, y):\n",
    "    \"\"\"\n",
    "    Calcula a raz√£o do ganho de informa√ß√£o (Information Gain Ratio)\n",
    "    \"\"\"\n",
    "    ig = calculate_information_gain(X_feature, y)\n",
    "    \n",
    "    # Entropia intr√≠nseca do atributo\n",
    "    unique_values, counts = np.unique(X_feature, return_counts=True)\n",
    "    probabilities = counts / len(X_feature)\n",
    "    intrinsic_entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "    \n",
    "    # Evita divis√£o por zero\n",
    "    if intrinsic_entropy == 0:\n",
    "        return 0\n",
    "    \n",
    "    return ig / intrinsic_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f82b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  \n",
    "# Os dados de treino foram gerados e gravados do \n",
    "#carregar os datasets do arquivo pickle\n",
    "with open('datasets.pkl', 'rb') as f:\n",
    "    datasets = pickle.load(f)\n",
    "\n",
    "X_train = datasets['X_train']\n",
    "y_train = datasets['y_train']\n",
    "X_train_scaled = datasets['X_train_scaled']\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara√ß√£o dos dados\n",
    "print(\"Preparando os dados...\")\n",
    "\n",
    "# Identifica a coluna alvo (assumindo que √© a √∫ltima coluna ou uma coluna espec√≠fica)\n",
    "# Vamos tentar identificar automaticamente a coluna alvo\n",
    "print(\"Colunas dispon√≠veis:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "\n",
    "# Se n√£o soubermos qual √© a coluna alvo, vamos assumir que √© a √∫ltima\n",
    "# Ou voc√™ pode especificar manualmente\n",
    "target_column = df.columns[-1]  # √öltima coluna como padr√£o\n",
    "print(f\"\\nUsando '{target_column}' como vari√°vel alvo\")\n",
    "\n",
    "# Separa features e target\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "print(f\"\\nShape das features: {X.shape}\")\n",
    "print(f\"Shape do target: {y.shape}\")\n",
    "print(f\"Valores √∫nicos no target: {y.nunique()}\")\n",
    "print(f\"Distribui√ß√£o do target:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0389ac4",
   "metadata": {},
   "source": [
    "## C√°lculo do Ganho de Informa√ß√£o\n",
    "\n",
    "Agora vamos calcular o ganho de informa√ß√£o para cada feature em rela√ß√£o √† vari√°vel alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo do ganho de informa√ß√£o para cada feature\n",
    "print(\"Calculando ganho de informa√ß√£o...\")\n",
    "\n",
    "# Para features cont√≠nuas, vamos discretiz√°-las primeiro\n",
    "def discretize_continuous_features(X, n_bins=10):\n",
    "    \"\"\"\n",
    "    Discretiza features cont√≠nuas em bins\n",
    "    \"\"\"\n",
    "    X_discrete = X.copy()\n",
    "    \n",
    "    for column in X.columns:\n",
    "        if X[column].dtype in ['float64', 'int64'] and X[column].nunique() > 20:\n",
    "            # Discretiza em bins\n",
    "            X_discrete[column] = pd.cut(X[column], bins=n_bins, labels=False)\n",
    "    \n",
    "    return X_discrete\n",
    "\n",
    "# Discretiza as features se necess√°rio\n",
    "X_discrete = discretize_continuous_features(X)\n",
    "\n",
    "# Calcula o ganho de informa√ß√£o para cada feature\n",
    "information_gains = {}\n",
    "information_gain_ratios = {}\n",
    "\n",
    "for column in X_discrete.columns:\n",
    "    # Remove valores NaN se houver\n",
    "    mask = ~(pd.isna(X_discrete[column]) | pd.isna(y))\n",
    "    feature_clean = X_discrete[column][mask].values\n",
    "    y_clean = y[mask].values\n",
    "    \n",
    "    if len(feature_clean) > 0:\n",
    "        ig = calculate_information_gain(feature_clean, y_clean)\n",
    "        igr = calculate_information_gain_ratio(feature_clean, y_clean)\n",
    "        \n",
    "        information_gains[column] = ig\n",
    "        information_gain_ratios[column] = igr\n",
    "\n",
    "# Cria DataFrame com os resultados\n",
    "ig_results = pd.DataFrame({\n",
    "    'Feature': list(information_gains.keys()),\n",
    "    'Information_Gain': list(information_gains.values()),\n",
    "    'Information_Gain_Ratio': list(information_gain_ratios.values())\n",
    "})\n",
    "\n",
    "# Ordena por ganho de informa√ß√£o\n",
    "ig_results = ig_results.sort_values('Information_Gain', ascending=False)\n",
    "\n",
    "print(\"\\nGanho de Informa√ß√£o por Feature (Top 15):\")\n",
    "print(ig_results.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ddd926",
   "metadata": {},
   "source": [
    "## C√°lculo do Mutual Information\n",
    "\n",
    "O Mutual Information mede a depend√™ncia estat√≠stica entre duas vari√°veis. Vamos usar a implementa√ß√£o do scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo do Mutual Information usando scikit-learn\n",
    "print(\"Calculando Mutual Information...\")\n",
    "\n",
    "# Prepara os dados para o mutual information\n",
    "# Remove valores NaN\n",
    "mask = ~(X.isna().any(axis=1) | pd.isna(y))\n",
    "X_clean = X[mask]\n",
    "y_clean = y[mask]\n",
    "\n",
    "# Para features categ√≥ricas, usa mutual_info_classif diretamente\n",
    "# Para features cont√≠nuas, tamb√©m funciona bem\n",
    "try:\n",
    "    # Calcula mutual information\n",
    "    mi_scores = mutual_info_classif(X_clean, y_clean, random_state=42)\n",
    "    \n",
    "    # Cria DataFrame com os resultados\n",
    "    mi_results = pd.DataFrame({\n",
    "        'Feature': X_clean.columns,\n",
    "        'Mutual_Information': mi_scores\n",
    "    })\n",
    "    \n",
    "    # Ordena por mutual information\n",
    "    mi_results = mi_results.sort_values('Mutual_Information', ascending=False)\n",
    "    \n",
    "    print(\"\\nMutual Information por Feature (Top 15):\")\n",
    "    print(mi_results.head(15))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro no c√°lculo do Mutual Information: {e}\")\n",
    "    print(\"Tentando com encoding das vari√°veis categ√≥ricas...\")\n",
    "    \n",
    "    # Se houver erro, tenta fazer encoding das vari√°veis categ√≥ricas\n",
    "    X_encoded = X_clean.copy()\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for column in X_encoded.columns:\n",
    "        if X_encoded[column].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            X_encoded[column] = le.fit_transform(X_encoded[column].astype(str))\n",
    "            label_encoders[column] = le\n",
    "    \n",
    "    # Tenta novamente\n",
    "    mi_scores = mutual_info_classif(X_encoded, y_clean, random_state=42)\n",
    "    \n",
    "    mi_results = pd.DataFrame({\n",
    "        'Feature': X_encoded.columns,\n",
    "        'Mutual_Information': mi_scores\n",
    "    })\n",
    "    \n",
    "    mi_results = mi_results.sort_values('Mutual_Information', ascending=False)\n",
    "    \n",
    "    print(\"\\nMutual Information por Feature (Top 15):\")\n",
    "    print(mi_results.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b25a6ea",
   "metadata": {},
   "source": [
    "## Compara√ß√£o e Visualiza√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9dbe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina os resultados para compara√ß√£o\n",
    "try:\n",
    "    # Merge dos resultados\n",
    "    combined_results = ig_results.merge(\n",
    "        mi_results, \n",
    "        on='Feature', \n",
    "        how='outer', \n",
    "        suffixes=('_IG', '_MI')\n",
    "    )\n",
    "    \n",
    "    print(\"Compara√ß√£o entre Information Gain e Mutual Information:\")\n",
    "    print(\"=\"*70)\n",
    "    print(combined_results.head(15))\n",
    "    \n",
    "    # Calcula correla√ß√£o entre as m√©tricas\n",
    "    if len(combined_results) > 1:\n",
    "        correlation = combined_results['Information_Gain'].corr(\n",
    "            combined_results['Mutual_Information']\n",
    "        )\n",
    "        print(f\"\\nCorrela√ß√£o entre Information Gain e Mutual Information: {correlation:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao combinar resultados: {e}\")\n",
    "    print(\"\\nResultados separados:\")\n",
    "    print(\"\\nInformation Gain:\")\n",
    "    if 'ig_results' in locals():\n",
    "        print(ig_results.head(10))\n",
    "    print(\"\\nMutual Information:\")\n",
    "    if 'mi_results' in locals():\n",
    "        print(mi_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Top 15 features por Information Gain\n",
    "plt.subplot(2, 2, 1)\n",
    "if 'ig_results' in locals() and len(ig_results) > 0:\n",
    "    top_ig = ig_results.head(15)\n",
    "    plt.barh(range(len(top_ig)), top_ig['Information_Gain'])\n",
    "    plt.yticks(range(len(top_ig)), top_ig['Feature'], fontsize=8)\n",
    "    plt.xlabel('Information Gain')\n",
    "    plt.title('Top 15 Features - Information Gain')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "# Subplot 2: Top 15 features por Mutual Information\n",
    "plt.subplot(2, 2, 2)\n",
    "if 'mi_results' in locals() and len(mi_results) > 0:\n",
    "    top_mi = mi_results.head(15)\n",
    "    plt.barh(range(len(top_mi)), top_mi['Mutual_Information'])\n",
    "    plt.yticks(range(len(top_mi)), top_mi['Feature'], fontsize=8)\n",
    "    plt.xlabel('Mutual Information')\n",
    "    plt.title('Top 15 Features - Mutual Information')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "# Subplot 3: Correla√ß√£o entre IG e MI (se dispon√≠vel)\n",
    "plt.subplot(2, 2, 3)\n",
    "try:\n",
    "    if 'combined_results' in locals() and len(combined_results) > 1:\n",
    "        plt.scatter(combined_results['Information_Gain'], \n",
    "                   combined_results['Mutual_Information'], \n",
    "                   alpha=0.7)\n",
    "        plt.xlabel('Information Gain')\n",
    "        plt.ylabel('Mutual Information')\n",
    "        plt.title('Information Gain vs Mutual Information')\n",
    "        \n",
    "        # Adiciona linha de tend√™ncia\n",
    "        z = np.polyfit(combined_results['Information_Gain'].fillna(0), \n",
    "                      combined_results['Mutual_Information'].fillna(0), 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(combined_results['Information_Gain'].fillna(0), \n",
    "                p(combined_results['Information_Gain'].fillna(0)), \n",
    "                \"r--\", alpha=0.8)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Dados n√£o dispon√≠veis\\npara correla√ß√£o', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "except Exception as e:\n",
    "    plt.text(0.5, 0.5, f'Erro: {str(e)[:50]}...', \n",
    "            ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "# Subplot 4: Distribui√ß√£o dos valores\n",
    "plt.subplot(2, 2, 4)\n",
    "try:\n",
    "    if 'ig_results' in locals() and 'mi_results' in locals():\n",
    "        plt.hist(ig_results['Information_Gain'], alpha=0.7, label='Information Gain', bins=20)\n",
    "        plt.hist(mi_results['Mutual_Information'], alpha=0.7, label='Mutual Information', bins=20)\n",
    "        plt.xlabel('Valor')\n",
    "        plt.ylabel('Frequ√™ncia')\n",
    "        plt.title('Distribui√ß√£o dos Valores')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Dados n√£o dispon√≠veis', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "except Exception as e:\n",
    "    plt.text(0.5, 0.5, f'Erro: {str(e)[:50]}...', \n",
    "            ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be063e1b",
   "metadata": {},
   "source": [
    "## Salvando os Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os resultados em arquivos CSV\n",
    "try:\n",
    "    # Salva Information Gain\n",
    "    if 'ig_results' in locals():\n",
    "        ig_results.to_csv('../dataset/information_gain_results.csv', index=False)\n",
    "        print(\"‚úÖ Resultados do Information Gain salvos em '../dataset/information_gain_results.csv'\")\n",
    "    \n",
    "    # Salva Mutual Information\n",
    "    if 'mi_results' in locals():\n",
    "        mi_results.to_csv('../dataset/mutual_information_results.csv', index=False)\n",
    "        print(\"‚úÖ Resultados do Mutual Information salvos em '../dataset/mutual_information_results.csv'\")\n",
    "    \n",
    "    # Salva resultados combinados se dispon√≠vel\n",
    "    if 'combined_results' in locals():\n",
    "        combined_results.to_csv('../dataset/combined_information_results.csv', index=False)\n",
    "        print(\"‚úÖ Resultados combinados salvos em '../dataset/combined_information_results.csv'\")\n",
    "    \n",
    "    print(\"\\nüìä Resumo da An√°lise:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if 'ig_results' in locals():\n",
    "        print(f\"‚Ä¢ Total de features analisadas: {len(ig_results)}\")\n",
    "        print(f\"‚Ä¢ Feature com maior Information Gain: {ig_results.iloc[0]['Feature']} ({ig_results.iloc[0]['Information_Gain']:.4f})\")\n",
    "    \n",
    "    if 'mi_results' in locals():\n",
    "        print(f\"‚Ä¢ Feature com maior Mutual Information: {mi_results.iloc[0]['Feature']} ({mi_results.iloc[0]['Mutual_Information']:.4f})\")\n",
    "    \n",
    "    if 'combined_results' in locals() and len(combined_results) > 1:\n",
    "        correlation = combined_results['Information_Gain'].corr(combined_results['Mutual_Information'])\n",
    "        print(f\"‚Ä¢ Correla√ß√£o entre IG e MI: {correlation:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao salvar resultados: {e}\")\n",
    "\n",
    "print(\"\\nüéâ An√°lise conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e245e2",
   "metadata": {},
   "source": [
    "## Interpreta√ß√£o dos Resultados\n",
    "\n",
    "### Information Gain vs Mutual Information\n",
    "\n",
    "**Information Gain:**\n",
    "- Mede a redu√ß√£o na entropia quando dividimos o dataset por um atributo\n",
    "- Valores mais altos indicam maior capacidade de discrimina√ß√£o\n",
    "- Favorece atributos com mais valores √∫nicos (pode ter vi√©s)\n",
    "\n",
    "**Mutual Information:**\n",
    "- Mede a depend√™ncia estat√≠stica entre vari√°veis\n",
    "- Mais robusto para diferentes tipos de vari√°veis\n",
    "- N√£o favorece atributos com mais valores √∫nicos\n",
    "\n",
    "### Como usar os resultados:\n",
    "1. **Sele√ß√£o de Features**: Use as features com maior IG ou MI para modelos de ML\n",
    "2. **Feature Engineering**: Combine ou transforme features com baixo IG/MI\n",
    "3. **An√°lise Explorat√≥ria**: Entenda quais vari√°veis s√£o mais informativas\n",
    "4. **Redu√ß√£o de Dimensionalidade**: Mantenha apenas as features mais importantes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
