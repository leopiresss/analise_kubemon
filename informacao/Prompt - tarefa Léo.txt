# Papel
> Monitor de Curso de Ciência de dados e Inteligência artificial

# Objetivo
> Descrever com precição o passo a passo do estudo que o professor pediu para o Léo e o Alexandre fazerem.
> Para cada passo, indique as ferramentas mais adequadas (pacotes python), a entrada e saída esperadas (dataframes, gráficos, etc).

# Fonte de dados
Essa é a transcrição de um vídeo de uma reunião de monitoria de TCC e trata sobre a análise de dois datasets e uma tarefa que será feita pelo Léo e o ALexandre analisando esses dois datasets e realizando treinamentos de modelos preditivos. 
Há erros na transcrição (como atriz quando deveria ser matriz, curaça quando deveria ser acurácia, etc), mas por alto ela trata da explicação de dois datasets de coleta de dados de ambiente docker para avaliar interferência entre containers rodando um trainamento simples svm e terasort de dados. O professor Eduardo solicitou a análise dos datasets, identificação de ganhos de informação das features deles, treinamento de modelos como árvore de decisão, acurácia, F1 score e curva ROC

A seguir está a transcrição da reunião, feita pelo whisper model turbo. Aproveite que o professor repediu algumas informações para compreender precisamente a proposta do estudo.
Realize a tarefa com precisão, atenção e gaste o tempo que for necessário.

# Transcrição da reunião:
Por exemplo, são duas, então são duas bases distintas. Treina lá um random forest em cima dela para fazer previsão. Tu tem um target lá, né? Eu pedi para o Pedro te entregar com o target. Sim, tem um target aqui. Você tem lá, acho que é degradado ou não, alguma coisa assim. É, tem interferência ou não. Isso. Tenta prever aquele target e avalia o modelo. Então pega lá um random forest, treina esse modelo do random forest, avalia qual foi a curaça dele, faz uma clínica confusão, faz uma curva rock, compara vários modelos, pega um random forest, uma árvore de decisão, uma MLP, por exemplo, e compara. Faz isso para esse dele e faz isso para o terraforte. Vamos dar uma olhada na curaça, porque essa base eu pedi para o Pedro gerar, mas não é exatamente a base que ele usou lá. A base que ele usou lá tinha que separar os nós, era bem mais chato. Aqui está tudo junto, então eu não sei como é que vai estar a curaça. Então avalia a curaça desses modelos, compara elas, e baseado nisso vamos ver o que a gente faz em cima. Então uma coisa que a gente pode fazer, por exemplo, é, a gente pode bater na tecla que a gente quer um modelo que foi treinado em uma job e ele consiga detectar na outra. Então a gente treina o modelo na FDM e ele tem que funcionar bem no terraforte, por exemplo, que é algo fácil de testar. E a gente consegue ter uma certa dificuldade para fazer classificação. A gente pode também trabalhar com seleção de features, então a gente tem que dar uma olhada nisso. Além dessa parte, Léo, como é uma base nova, minha sugestão é tu pegar isso, para cada dataset, né, se tem dois, calcula o ganho de informação das features, faz um gráfico dela, do ganho de informação. Porque pode ser que tenha alguma feature com ganho de informação altíssimo, a gente vai ter que remover. mas pelo menos te tira do zero, tu consegue fazer uma análise inicial e tu consegue ter isso com o TCC também, porque, querendo ou não, é uma base muito próxima da base que tu iria montar, né, não deu tempo. É. Então tu consegue sair do zero. E como tu tem o Alexandre, eu não sei mais quem está na tua equipe, mas tu consegue te ajudar também, né. Opa, bom professor. Sim, aí faz aqui, professor. Bom. É. Essa, eu sei que ia dar uma olhada no artigo dele, professor, mas assim, penso que eu estou entendendo, não preciso, é... Não precisa entender o domínio, né. Eu posso te explicar o domínio ali, rapidão. Abre o artigo lá, o original. O original tá aqui. Tá aqui. Tem uma figura lá que ela passa bem a ideia do que foi feito. Eu cheguei a ler e entendi algumas coisas, mas, por favor, pode citar. É, do Problem Statement. Eu acho que ele é uma figura, deve ser dois, três, alguma coisa assim. É três. É três, eu acho. É três. O que ele fez? Qual que era o problema que ele queria tratar lá, né, João? Ele fazia deploy de aplicações no Kubernetes, Docker no Kubernetes, e ele queria saber quando que tinha um Docker interferindo na performance do outro. Certo? Então era isso que ele queria fazer. Qual era o cenário? Ele assumia que ele tava fazendo deploy de aplicações de Big Data. Então é por isso que você tem uma aplicação lá que eu acho que é o Terasort, e outro que é o FBM. Que cenário que é esse? Ele tem uma massa de dados no HDFS. Vocês tiveram Big Data comigo, né? Sim. Não sei se o Alexandre teve. Sim, sim. Ele tinha uma massa de dados no HDFS. Naquele gráfico tem lá, né, Leandro? É, Leandro. Então, o que a job do Terasort faz, né? Não, não. Na figura 3, né? Na figura 3 mesmo. Ele tinha uma massa de dados armazenada no HDFS, e ele queria ordenar essa massa de dados. Isso que o Terasort faz. Então é uma job que lê do disco, ordena e escreve no disco de novo. E você tinha outra job que era o SVM. O que a job do SVM fazia? Ela lê uma base de cena no HDFS, treina um SVM, e escreve o modelo no HDFS de volta. Então essa job levava ali um minuto, um minuto e pouco, pra fazer. Certo? O que que ele fez aí? Em paralelo, ele rodava o workload. Então se tu olhar essa tabela que tá aí em cima. Então era, por exemplo, HTTPERS, era um Docker que ficava recebendo a requisição de HTTP. Sysbench, era um Docker que fazia stress de CPU. Enquanto ele rodava o Terasort, tinha outros Dockers rodando HTTP, Perf, Sysbench, Hiveint, e esse tipo de coisa. Então tava tendo uma interferência externa a job que ele executava no Big Data. O que que ele queria fazer? Ele queria detectar quando que a aplicação, a performance, do meu Terasort, rodando lá, por exemplo, tava degradada. Como que ele fez isso? Então como é que ele define o que é degradado e o que não é? Ele rodou o Terasort várias vezes sem ter nenhuma job em paralelo. Então era só o Terasort, não é isso? E vamos supor que esse Terasort levou um minuto, em média. E o desvio padrão foi muito baixo. Então, sei lá, levava de 50 segundos até 70. Era essa média. E daí ele rodou essa job com os workloads em paralelo. E ele percebeu que a job, em vez de levar 60 segundos, ela levava 3 minutos. Então ele assumia no cenário que a média mais o desvio padrão do tempo da job fosse maior, ele assumia que era degradado. Então se tu pegar o teu CSV lá, o cenário que ele fala que tá degradado é o cenário em que o tempo pra rodar a job for maior do que a média mais o desvio padrão. porque significa que ele tá muito fora da curva aquele tempo. Então é o cenário que tá degradado. Certo? O maior que a média... A média mais o desvio padrão do tempo da execução da job. A gente não vai usar isso, né? É, não vai usar. É só pra ele entender o cenário que foi feito. Então, aí como é que você... Como é que ele fez aí? Ele coletou informações dos dockers de uso de CPU, de escorrer, e montou aquela base que você tem. Que é aquele CSV que tu tem. E baseado nesse CSV, ele tenta detectar se tá degradado ou não. Ou seja, se é um cenário que a job vai demorar mais tempo do que devia ou não. É isso. A proposta dele. Ah, um CSV retrata o Random Forest e o outro CSV retrata o SVM, né? Não, acho que é um é o SVM e o outro é o TeraSort, né? Que ele lê o arquivo do disco, ordena e escreve de volta. Então são duas jobs distintas. O TeraSort é isso. Ele lê, sei lá, um Tera do disco, ordena esse um Tera e escreve de volta. E o SVM, ele treina o modelismo mesmo. Ah, ele treina... Ah, o SVM é baseado no TeraSort. Então tem que usar o TeraSort, né? Não, não. São duas jobs diferentes, Léo. O arquivo que tem o SVM, ele rodou uma job que treina o modelo do SVM. É essa carga que ele tá gerando. Ah, certo. E o arquivo que se chama TeraSort é uma job que ele rodou que faz o TeraSort. Ela ordena um arquivo grande. É isso que ele faz. Qual que é a minha sugestão agora? Só pra bater o Marcelo. Você vai ter que pegar o arquivo do SVM, treinar o modelo do Randall Forest, por exemplo. Treina vários modelos lá e faz a comparação clássica. Compara a curva Rock, faz uma matriz de confusão, vê as métricas de acurácia, esse tipo de coisa. E faz a mesma coisa pro outro arquivo do TeraSort. Aí vamos dar uma olhada nas acurácias, vamos ver a performance, vamos ver se tem o que que a gente melhora lá. A minha opinião é que muito provavelmente a acurácia seja altíssima. Porque deve ter atributos com ganho de informação alto. A gente vai ter que fazer isso. Então também pega a base lá, Léo e Alexandre, calcula o ganho de informação das features e faz um gráfico. Só pra gente dar uma olhada. Então pode ser que tenha atributos que a gente tem que remover. Mas eu, seu TCC, seria isso. Seria essa avaliação mais a fundo da base. Tá? Você pode escrever seu protótipo ainda no TCC, mas não sei se vai dar tempo. Oi, Eduardo. Oi. Eu acho que se ele pegasse a parte do disco, da escrita em disco e tentasse prever como isso interfere na performance, na degradação, já dava o trabalho dele de TCC. Não precisava ir construir um modelo. É muito rápido, né? É muito rápido treinar um modelo. Ele não tinha massa pronta. É um fit predictor. Não tem muito segredo. O Léo vai ter que separar e serenamente o teste de validação, né? É só isso. No caso dele, do TCC, agora, se ele conseguir fazer isso direitinho, como ele quer fazer uma predição, com base no modelo do disco, ele prevê a carga do sistema. A carga do sistema significa um impacto na memória e na CPU, porque ele tem essas informações, né? porque ali, Léo, era uma interferência externa que a gente colocava cargas diferentes externas pra ver o impacto disso. Você não precisa se preocupar com isso que o Eduardo explicou aí. Você vai pegar as features, olhar ou o disco, ou a CPU, ou a memória e ver o impacto do um no outro, o outro no um, e aí sai até o TCC. Não precisa fazer mais que isso, não, Eduardo. Mas, assim, só pra... Não, mas a parte de predição também é rápida. Então, vamos só bater o martelo pro TCC. Você vai pegar a base do SVM, separe em treinamento, teste e validação, Léo. E Alexandre? 40, 30, 30, 30. 40, 30, 30, de maneira aleatória considerando a distribuição das classes. E tem que ser o sample. Isso. Treina vários modelos, Léo e Alexandre, SVM, Decision Tree, escolhe vários lá e treina. Carme. Isso. Calcula as métricas lá de performance, matriz de mudão, curva rock, F1. Isso. E por fim, calcula o ganho de informação das features. Isso é rapidíssimo de vocês fazer, vocês já fizeram. Então não tem muito segredo. Beleza? Beleza. Acho que o senhor se o Alexandre tem alguma dúvida também, não sei quem vai gastar mais tempo nisso. Não, não vai tentar. A gente vai fazer junto aqui, professor. Vamos fazer junto. Na linha do TCC deles, eu acho que teria que botar regressão também, do tempo. que vai fazer um pouco mais que vai fazer um pouco mais que vai fazer um pouco mais que vai fazer um pouco mais que vai fazer um pouco mais que vai fazer um pouco mais de que vai fazer. Então, vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá. Vamos lá.
